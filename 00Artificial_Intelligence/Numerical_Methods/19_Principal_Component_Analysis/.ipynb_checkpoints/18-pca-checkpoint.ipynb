{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm as cm\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=2)\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning and data science, Principal Component Analysis (PCA) is a method of analysing datasets to obtain an orthogonal basis that best captures most of the variance of our data.  It is often used to remove extra features or dimensions in large-dimensional datasets, because in a similar vein to SVD, PCA will give us a set of axes with both large contributions and small contributions to our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Breast cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data set for characteristics of tumor cells (this is the same dataset used for the MP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\"radius\", \"texture\", \"perimeter\", \"area\",\n",
    "          \"smoothness\", \"compactness\", \"concavity\",\n",
    "          \"concave points\", \"symmetry\", \"fractal dimension\"];\n",
    "stats = [\"(mean)\", \"(stderr)\", \"(worst)\"]\n",
    "labels = [\"patient ID\", \"Malignant/Benign\"]\n",
    "\n",
    "for p in params:\n",
    "    for s in stats:\n",
    "        labels.append(p + \" \" + s)\n",
    "\n",
    "tumor_data = pd.io.parsers.read_csv(\"breast-cancer-train.dat\",header=None,names=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Creating a smaller subset of points:\n",
    "Selecting a subset of the data for better visualization and understanding of the method. We will start with six patients and only two of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame(tumor_data[[\"Malignant/Benign\", 'smoothness (mean)', 'radius (mean)']][272:278])\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = sns.lmplot('smoothness (mean)', 'radius (mean)', new_data,  hue=\"Malignant/Benign\", scatter_kws={\"s\": 180}, fit_reg=False, height=8)\n",
    "ax = g1.axes[0,0]\n",
    "ax.axis('equal')\n",
    "\n",
    "# This code snippet is plotting the labels\n",
    "for i in range(272,278):\n",
    "    x = new_data['smoothness (mean)'][i] + 0.1\n",
    "    y = new_data['radius (mean)'][i] + 0.1\n",
    "    ax.text(x,y,str(i),horizontalalignment='left',size='medium', color='black', weight='semibold', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Shift the dataset to center the data:\n",
    "The first step is to determine the \"center\" of the dataset (the mean value of each feature):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_smooth = new_data['smoothness (mean)'].mean()\n",
    "mean_radius = new_data['radius (mean)'].mean()\n",
    "\n",
    "print(mean_smooth,mean_radius)\n",
    "\n",
    "print(new_data['smoothness (mean)'].std(),new_data['radius (mean)'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = sns.lmplot('smoothness (mean)', 'radius (mean)', new_data,  hue=\"Malignant/Benign\", scatter_kws={\"s\": 180}, fit_reg=False, height=8)\n",
    "ax = g1.axes[0,0]\n",
    "plt.xlim(-2,24)\n",
    "plt.ylim(-2,24)\n",
    "for i in range(272,278):\n",
    "    x = new_data['smoothness (mean)'][i] + 0.1\n",
    "    y = new_data['radius (mean)'][i] + 0.1\n",
    "    ax.text(x,y,str(i),horizontalalignment='left',size='medium', color='black', weight='semibold', fontsize=16)\n",
    "        \n",
    "ax.scatter(mean_smooth,mean_radius,  s=180, c='r', marker=(5, 2))\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "ax.axhline(y=mean_radius, color='m',linestyle='--')\n",
    "ax.axvline(x=mean_smooth, color='m',linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we \"center\" the dataset, such that each feature has zero mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['smoothness (zero mean)'] = new_data['smoothness (mean)'] - new_data['smoothness (mean)'].mean()\n",
    "new_data['radius (zero mean)'] = new_data['radius (mean)'] - new_data['radius (mean)'].mean()\n",
    "\n",
    "print(new_data['smoothness (zero mean)'].mean())\n",
    "print(new_data['radius (zero mean)'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['smoothness (zero mean)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the centered data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = sns.lmplot('smoothness (zero mean)', 'radius (zero mean)', new_data, hue=\"Malignant/Benign\", scatter_kws={\"s\": 180}, fit_reg=False, height=8)\n",
    "ax = g1.axes[0,0]\n",
    "ax.axis('equal')\n",
    "\n",
    "for i in range(272,278):\n",
    "    x = new_data['smoothness (zero mean)'][i] + 0.1\n",
    "    y = new_data['radius (zero mean)'][i] + 0.1\n",
    "    ax.text(x,y,str(i),horizontalalignment='left',size='medium', color='black', weight='semibold', fontsize=16)\n",
    "\n",
    "ax.scatter(0,0,  s=200, c='r', marker=(5, 2))\n",
    "\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Get covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA wants to find the directions of maximum variance. For that, we will need to first define the covariance matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ M $: total number of data points\n",
    "\n",
    "$ N $ : number of features\n",
    "\n",
    "$Cov({\\bf A}) = \\frac{1}{M-1} {\\bf A}^T {\\bf A} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centered data\n",
    "A = new_data[['smoothness (zero mean)', 'radius (zero mean)']]\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "M,N = A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the definition\n",
    "cov_matrix = (1/(M-1))*A.T@A\n",
    "print(cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or using python built-in function\n",
    "A.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variances are on the diagonal (co-variance of a variable with itself), and the sum of the 2 values is the overall variability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(cov_matrix).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "7.53951/31.35945"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA replaces the original variables with new variables, called principal components, which are orthogonal (i.e. they have zero covariations) and have variances in decreasing order. To accomplish this, we will use the diagonalization of the covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l,u = la.eig(cov_matrix)\n",
    "print(l)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$cov({\\bf A}) = \\begin{bmatrix} -0.40237101 & -0.91547669 \\\\ -0.91547669 &  0.40237101 \\end{bmatrix} \\begin{bmatrix} 27.71798127 & 0 \\\\ 0 &  3.6414725  \\end{bmatrix} \\begin{bmatrix} -0.40237101 & -0.91547669 \\\\ -0.91547669 &  0.40237101 \\end{bmatrix}^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "27.717981/31.3594537"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the diagonal sum is still 31.359, which says that the two components account for all the variability.\n",
    "\n",
    "\n",
    "PCA finds, in the data space, the dimension (direction) with the largest variance out of the overall variance.\n",
    "\n",
    "\n",
    "In this example, if we reduce the dimension space to include only one variable, the first principal component 27.718, accounts for 88% of the variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the largest eigenvalue of the covariance matrix corresponds to the largest variance of the dataset, and the associated eigenvector is the direction of maximum variance. For our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = sns.lmplot('smoothness (zero mean)', 'radius (zero mean)', new_data, hue=\"Malignant/Benign\", scatter_kws={\"s\": 180}, fit_reg=False, height=8)\n",
    "ax = g1.axes[0,0]\n",
    "ax.axis('equal')\n",
    "\n",
    "for i in range(272,278):\n",
    "    x = new_data['smoothness (zero mean)'][i] + 0.1\n",
    "    y = new_data['radius (zero mean)'][i] + 0.1\n",
    "    ax.text(x,y,str(i),horizontalalignment='left',size='medium', color='black', weight='semibold', fontsize=16)\n",
    "\n",
    "ax.scatter(0,0,  s=200, c='r', marker=(5, 2))\n",
    "\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "\n",
    "s = 3\n",
    "\n",
    "J = 0 # choice of principal direction\n",
    "x = u[0,J]\n",
    "y = u[1,J]\n",
    "ax.arrow(0,0,s*x,s*y,color='black',head_width=0.1, head_length=0.1, fc='r', ec='r', lw=5)\n",
    "\n",
    "J = 1 # choice of principal direction\n",
    "x = u[0,J]\n",
    "y = u[1,J]\n",
    "ax.arrow(0,0,s*x,s*y,color='black',head_width=0.1, head_length=0.1, fc='m', ec='m', lw=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the eigenvectors of ${\\bf A}^T{\\bf A}$ are the right singular vectors of ${\\bf A}$, or the columns of ${\\bf V}$ from the SVD decomposition of ${\\bf A}$ (or the rows of V transpose). \n",
    "\n",
    "Hence, instead of having to calculate the covariance matrix and solve an eigenvalue problem, we will instead get the reduced form of the SVD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "# variances = eig(covariance) = singular values squared\n",
    "variances = S**2\n",
    "\n",
    "print(variances)\n",
    "\n",
    "# principal directions\n",
    "pc1_vec = Vt[0,:]\n",
    "pc2_vec = Vt[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we don't need to worry about the constant value from the covariance matrix that we are disregarding. The variance values change, but their proportionality remains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances[0]/variances.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Plotting the principal directions using the singular right vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = sns.lmplot('smoothness (zero mean)', 'radius (zero mean)', new_data, hue=\"Malignant/Benign\", scatter_kws={\"s\": 180}, fit_reg=False, height=8)\n",
    "ax = g1.axes[0,0]\n",
    "\n",
    "for i in range(272,278):\n",
    "    x = new_data['smoothness (zero mean)'][i] + 0.1\n",
    "    y = new_data['radius (zero mean)'][i] + 0.1\n",
    "    ax.text(x,y,str(i),horizontalalignment='left',size='medium', color='black', weight='semibold', fontsize=16)\n",
    "\n",
    "ax.scatter(0,0,  s=200, c='r', marker=(5, 2))\n",
    "ax.axis('equal')\n",
    "\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "\n",
    "s = 3\n",
    "\n",
    "x = pc1_vec[0]\n",
    "y = pc1_vec[1]\n",
    "ax.arrow(0,0,s*x,s*y,color='black',head_width=0.1, head_length=0.1, fc='r', ec='r', lw=5)\n",
    "\n",
    "x = pc2_vec[0]\n",
    "y = pc2_vec[1]\n",
    "ax.arrow(0,0,s*x,s*y,color='black',head_width=0.1, head_length=0.1, fc='m', ec='m', lw=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Cumulative explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pc1_vec)\n",
    "print(pc2_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the largest variance is 138.6 and the direction of this principal component is given by the vector `pc1_vec`.\n",
    "\n",
    "The second largest variance is 18.2 and the direction of this principal component is given by the vector `pc2_vec`.\n",
    "\n",
    "In a general problem, we would have many principal components. How can we easily visualize these components and decide how many we will keep in our reduced feature space? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(variances)\n",
    "var_exp = [(i / tot)*100 for i in variances]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "plt.bar(range(len(var_exp)),var_exp, align='center', label='individual explained variance')\n",
    "plt.step(range(len(var_exp)), cum_var_exp, 'r', where='mid', label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xstar=(A@Vt.T).values\n",
    "new_data['pc1'] = Xstar[:,0]  \n",
    "new_data['pc2'] = Xstar[:,1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = sns.lmplot('pc1', 'pc2', new_data, hue=\"Malignant/Benign\", fit_reg=False, height=8, scatter_kws={\"s\": 180})\n",
    "ax = g1.axes[0,0]\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "ax.axis('equal')\n",
    "\n",
    "\n",
    "for i in range(272,278):\n",
    "    x = new_data['pc1'][i] + 0.1\n",
    "    y = new_data['pc2'][i] + 0.1\n",
    "    ax.text(x,y,str(i),horizontalalignment='left',size='medium', color='black', weight='semibold', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Complete dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since we have only two features here, PCA is not really helping! Let's go back to original example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_large = tumor_data.iloc[:,2:].values\n",
    "\n",
    "type(A_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_large.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Center the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_large.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = (A_large - A_large.mean(axis=0))\n",
    "# print(X.mean(axis=0))\n",
    "\n",
    "X = (A_large - A_large.mean(axis=0))/A_large.std(axis=0)\n",
    "print(X.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s,vt = la.svd(X,full_matrices=False)\n",
    "\n",
    "variances = s**2\n",
    "\n",
    "tot = sum(variances)\n",
    "var_exp = [(i / tot)*100 for i in variances]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "plt.bar(range(len(var_exp)),var_exp, align='center', label='individual explained variance')\n",
    "plt.step(range(len(var_exp)), cum_var_exp, 'r', where='mid', label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_var_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the most important principal directions, and transform the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vstar = vt[:3,:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xstar=(X@Vstar) # change of basis\n",
    "\n",
    "Xstar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_data_new = tumor_data.copy()\n",
    "\n",
    "tumor_data_new['pc1'] = Xstar[:,0]  \n",
    "tumor_data_new['pc2'] = Xstar[:,1]  \n",
    "tumor_data_new['pc3'] = Xstar[:,2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = sns.lmplot('pc1', 'pc2', tumor_data_new, hue=\"Malignant/Benign\", fit_reg=False, height=8, scatter_kws={\"s\": 180})\n",
    "ax = g1.axes[0,0]\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "ax.axis('equal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "threedee = plt.figure().gca(projection='3d')\n",
    "threedee.scatter(tumor_data_new['pc1'], tumor_data_new['pc2'], tumor_data_new['pc3'],c = tumor_data_new[\"Malignant/Benign\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the weight for each feature in the first principal component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tumor_data_new.columns[2:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = vt.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(feature_names,V[:,0])\n",
    "plt.xticks(rotation=90);\n",
    "plt.title('importance of each attribute in ${\\\\bf p}_1$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Food consumption in UK (using sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from:\n",
    "http://setosa.io/ev/principal-component-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.io.parsers.read_csv(\"UK_foods.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Picture1.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = data['Unnamed: 0'].values.tolist()\n",
    "print(headers)\n",
    "\n",
    "new_data = data.drop(['Unnamed: 0'], axis=1)\n",
    "new_data.head()\n",
    "\n",
    "regions = new_data.columns.values.tolist()\n",
    "print(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = pd.DataFrame(new_data.values.T,columns=headers)\n",
    "food['region'] = regions\n",
    "food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is when we want to try PCA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing PCA without scaling the data (to match the results from the website)\n",
    "#X = pd.DataFrame(food[headers], columns=headers)\n",
    "# In general, PCA scales the variables to zero-mean (use line below to scale)\n",
    "X = pd.DataFrame(scale(food[headers]), columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(X)\n",
    "pca_samples = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_exp = pca.explained_variance_ratio_\n",
    "plt.bar(range(len(var_exp)),var_exp, align='center', label='individual explained variance');\n",
    "plt.ylabel('Explained variance ratio');\n",
    "plt.xlabel('Principal components');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pd.DataFrame(pca.components_, columns = headers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar(headers,components.values[0])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('influence of original variables(food) upon pc1')\n",
    "plt.figure()\n",
    "plt.bar(headers,components.values[1])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('influence of original variables(food) upon pc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xstar = pd.DataFrame(pca_samples,columns=['pc1','pc2','pc3','pc4'])\n",
    "Xstar['region'] = regions\n",
    "Xstar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(x=\"pc1\",y=\"region\", data=Xstar, jitter=0.05, linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure()\n",
    "ax = sns.lmplot('pc1', 'pc2',Xstar,hue='region', fit_reg=False)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('pc1')\n",
    "plt.ylabel('pc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
